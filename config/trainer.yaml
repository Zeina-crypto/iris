defaults:
  - _self_
  - tokenizer: default
  - world_model: default
  # actor_critic: default
  - env: default
  - datasets: default

wandb:
  mode: online #disabled #online
  project: iris
  entity: null
  name: null 
  group: null
  tags: null
  notes: null

initialization:
  path_to_checkpoint: '/space/zboucher/model_checkpoint_epoch_100.pt' 
  load_tokenizer: True 
  load_world_model: False
  #load_actor_critic: False

common:
  epochs: 60
  device: cuda:0
  batch_size: 512
  do_checkpoint: True
  seed: 0
  obs_time: 7 
  pred_time: 9 
  time_interval: 30
  sequence_length: ${world_model.max_blocks}
  resume: False

collection:
  train:
    num_envs: 1
    stop_after_epochs: 60
    num_episodes_to_save: 4
    config:
      epsilon: 0.01
      should_sample: True
      temperature: 1.0
      num_steps: 200
  
  test:
    num_envs: 8 # was 8 
    num_episodes_to_save: ${collection.train.num_episodes_to_save}
    config:
      epsilon: 0.0
      should_sample: True
      temperature: 0.5
      num_episodes: 4
    #  burn_in: ${training.actor_critic.burn_in}

training:
  should: True
  learning_rate: 0.0001
  tokenizer:
    batch_num_samples: 1 #2  # batch_size
    grad_acc_steps: 16 #8
    max_grad_norm: 10.0
    start_after_epochs: 100 #was 5
    stop_after_epochs: 200
  #  steps_per_epoch: 200
  world_model:
    batch_num_samples: 1 #2 # batch_size
    grad_acc_steps: 16 #8
    max_grad_norm: 10.0
    weight_decay: 0.01
    start_after_epochs: 0 # was 25
    stop_after_epochs: 60
  #  steps_per_epoch: 200
  # actor_critic:
    # batch_num_samples: 64
    # grad_acc_steps: 1
    # max_grad_norm: 10.0
    # start_after_epochs: 50
    # steps_per_epoch: 200
    # imagine_horizon: ${common.sequence_length}
    # burn_in: 20
    # gamma: 0.995
    # lambda_: 0.95
    # entropy_weight: 0.001

evaluation:
  should: True
  every: 2
  tokenizer:
    batch_num_samples: 1 #2
    start_after_epochs: ${training.tokenizer.start_after_epochs}
    save_reconstructions: True
    stop_after_epochs: 5
  world_model:
    batch_num_samples: 1 #2
    start_after_epochs: ${training.world_model.start_after_epochs}
    stop_after_epochs: 60
    #save_generations: True
