_target_: models.TransformerConfig
tokens_per_block: 16
max_blocks: 16 # length of the sequence
attention: 'causal'
num_layers: 6
num_heads: 8
embed_dim: 512
embed_pdrop: 0.1
resid_pdrop: 0.1
attn_pdrop: 0.1

