{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credits to https://github.com/CompVis/taming-transformers\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from dataset import Batch\n",
    "from lpips import LPIPS\n",
    "from nets import Encoder, Decoder\n",
    "from utils import LossWithIntermediateLosses\n",
    "\n",
    "\n",
    "batch=1\n",
    "@dataclass\n",
    "class TokenizerEncoderOutput:\n",
    "    z: torch.FloatTensor\n",
    "    z_quantized: torch.FloatTensor\n",
    "    tokens: torch.LongTensor\n",
    "\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, encoder: Encoder, decoder: Decoder, with_lpips: bool) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.encoder = encoder\n",
    "        self.pre_quant_conv = torch.nn.Conv2d(encoder.config.z_channels, embed_dim, 1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, decoder.config.z_channels, 1)\n",
    "        self.decoder = decoder\n",
    "        self.embedding.weight.data.uniform_(-1.0 / vocab_size, 1.0 / vocab_size)\n",
    "        self.lpips = LPIPS().eval() if with_lpips else None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"tokenizer\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> Tuple[torch.Tensor]:\n",
    "        outputs = self.encode(x, should_preprocess)\n",
    "        decoder_input = outputs.z + (outputs.z_quantized - outputs.z).detach()\n",
    "        reconstructions = self.decode(decoder_input, should_postprocess)\n",
    "        return outputs.z, outputs.z_quantized, reconstructions\n",
    "\n",
    "    def compute_loss(self, x, **kwargs: Any) -> LossWithIntermediateLosses:\n",
    "        assert self.lpips is not None\n",
    "        observations = x\n",
    "        z, z_quantized, reconstructions = self(observations, should_preprocess=False, should_postprocess=False)\n",
    "\n",
    "        # Codebook loss. Notes:\n",
    "        # - beta position is different from taming and identical to original VQVAE paper\n",
    "        # - VQVAE uses 0.25 by default\n",
    "        beta = 1.0\n",
    "        commitment_loss = (z.detach() - z_quantized).pow(2).mean() + beta * (z - z_quantized.detach()).pow(2).mean()\n",
    "\n",
    "        reconstruction_loss = torch.abs(observations - reconstructions).mean()\n",
    "        perceptual_loss = torch.mean(self.lpips(observations, reconstructions))\n",
    "\n",
    "        return LossWithIntermediateLosses(commitment_loss=commitment_loss, reconstruction_loss=reconstruction_loss,perceptual_loss=perceptual_loss)\n",
    "\n",
    "    def encode(self, x: torch.Tensor, should_preprocess: bool = False) -> TokenizerEncoderOutput:\n",
    "        if should_preprocess:\n",
    "            x = self.preprocess_input(x)\n",
    "        #print(\"Shape of x:\", x.shape)\n",
    "        shape = x.shape  # (..., C, H, W)\n",
    "        x = x.view(-1, *shape[-3:])\n",
    "        #print(\"Shape of x as (x_view):\", x.shape)\n",
    "        z = self.encoder(x)\n",
    "        #print(\"Shape of z:\",z.shape)\n",
    "        z = self.pre_quant_conv(z)\n",
    "        b, e, h, w = z.shape\n",
    "        z_flattened = rearrange(z, 'b e h w -> (b h w) e')\n",
    "        #print(\"Shape of z_flattend:\",z_flattened.shape)\n",
    "        dist_to_embeddings = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "\n",
    "        tokens = dist_to_embeddings.argmin(dim=-1)\n",
    "        #print(\"Shape of tokens:\",tokens.shape)\n",
    "        z_q = rearrange(self.embedding(tokens), '(b h w) e -> b e h w', b=b, e=e, h=h, w=w).contiguous()\n",
    "        #print(\"Shape of z_q:\",z_q.shape)\n",
    "        # Reshape to original\n",
    "        z = z.reshape(*shape[:-3], *z.shape[1:])\n",
    "        #print(\"Shape of z:\", z.shape)\n",
    "        z_q = z_q.reshape(*shape[:-3], *z_q.shape[1:])\n",
    "        #print(\"Shape of reshaped z_q:\", z_q.shape)\n",
    "        tokens = tokens.reshape(*shape[:-3], -1)\n",
    "        #print(\"Shape of tokens:\", tokens.shape)\n",
    "\n",
    "        return TokenizerEncoderOutput(z, z_q, tokens)\n",
    "\n",
    "    def decode(self, z_q: torch.Tensor, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        shape = z_q.shape  # (..., E, h, w)\n",
    "        z_q = z_q.view(-1, *shape[-3:])\n",
    "        z_q = self.post_quant_conv(z_q)\n",
    "        rec = self.decoder(z_q)\n",
    "        rec = rec.reshape(*shape[:-3], *rec.shape[1:])\n",
    "        if should_postprocess:\n",
    "            rec = self.postprocess_output(rec)\n",
    "        return rec\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_decode(self, x: torch.Tensor, should_preprocess: bool = False, should_postprocess: bool = False) -> torch.Tensor:\n",
    "        z_q = self.encode(x, should_preprocess).z_quantized\n",
    "        return self.decode(z_q, should_postprocess)\n",
    "\n",
    "    def preprocess_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x is supposed to be channels first and in [0, 1]\"\"\"\n",
    "        return x.mul(2).sub(1)\n",
    "\n",
    "    def postprocess_output(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"y is supposed to be channels first and in [-1, 1]\"\"\"\n",
    "        return y.add(1).div(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "@dataclass\n",
    "class EncoderDecoderConfig:\n",
    "    resolution: int\n",
    "    in_channels: int\n",
    "    z_channels: int\n",
    "    ch: int\n",
    "    ch_mult: List[int]\n",
    "    num_res_blocks: int\n",
    "    attn_resolutions: List[int]\n",
    "    out_ch: int\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : shape of latent is (2048, 8, 8).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = Encoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=2048,\n",
    "                                        ch=64,\n",
    "                                        ch_mult= [1, 1, 1, 1,1,1],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [8, 16],\n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.0))\n",
    "decoder=Decoder(EncoderDecoderConfig(resolution=256,\n",
    "                                       in_channels=1,\n",
    "                                        z_channels=2048,\n",
    "                                        ch=64,\n",
    "                                        ch_mult= [1, 1, 1, 1,1,1],\n",
    "                                        num_res_blocks= 2,\n",
    "                                        attn_resolutions= [8, 16],\n",
    "                                        out_ch= 1,\n",
    "                                        dropout= 0.0))\n",
    "vocab_size =64 # Replace with the actual vocabulary size\n",
    "embed_dim = 512  # Replace with the desired embedding dimension\n",
    "tokenizer = Tokenizer(vocab_size, embed_dim, encoder, decoder, with_lpips=True)\n",
    "#print(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.0001 # cfg training file \n",
    "optimizer_tokenizer = torch.optim.Adam(tokenizer.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            #image = 2*image-1 #normalize to [-1,1]\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32183 3493 3560\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "batch_size=1\n",
    "\n",
    "df_train = pd.read_csv('/users/zboucher/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('/users/zboucher/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('/users/zboucher/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/zboucher/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/zboucher/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/zboucher/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('/users/zboucher/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "data_list = [dataset_train_aa, dataset_train_dw, dataset_train_del, dataset_train_re]\n",
    "train_aadedwre = torch.utils.data.ConcatDataset(data_list)\n",
    "\n",
    "print(len(dataset_train), len(dataset_test), len(dataset_vali))\n",
    "loaders = { 'train' :DataLoader(train_aadedwre, batch_size, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size, shuffle=False, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size, shuffle=True, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size, shuffle=False, num_workers=8),\n",
    "          }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 0: Total Loss = 1.1524\n",
      "Epoch 0: Total Loss = 1.1757\n",
      "Epoch 0: Total Loss = 1.1320\n",
      "Epoch 0: Total Loss = 1.1416\n",
      "Epoch 0: Total Loss = 1.1564\n",
      "Epoch 0: Total Loss = 1.1336\n",
      "Epoch 0: Total Loss = 1.1260\n",
      "Epoch 0: Total Loss = 1.1518\n",
      "Epoch 0: Total Loss = 1.1753\n",
      "Epoch 0: Total Loss = 1.1904\n",
      "Epoch 0: Total Loss = 1.1400\n",
      "Epoch 0: Total Loss = 1.1562\n",
      "Epoch 0: Total Loss = 1.1580\n",
      "Epoch 0: Total Loss = 1.1345\n",
      "Epoch 0: Total Loss = 1.1617\n",
      "Epoch 0: Total Loss = 1.1492\n",
      "Epoch 0: Total Loss = 1.1375\n",
      "Epoch 0: Total Loss = 1.1515\n",
      "Epoch 0: Total Loss = 1.1668\n",
      "Epoch 0: Total Loss = 1.1463\n",
      "Epoch 0: Total Loss = 1.1774\n",
      "Epoch 0: Total Loss = 1.1618\n",
      "Epoch 0: Total Loss = 1.1662\n",
      "Epoch 0: Total Loss = 1.1714\n",
      "Epoch 0: Total Loss = 1.1550\n",
      "Epoch 0: Total Loss = 1.1747\n",
      "Epoch 0: Total Loss = 1.1406\n",
      "Epoch 0: Total Loss = 1.1699\n",
      "Epoch 0: Total Loss = 1.1480\n",
      "Epoch 0: Total Loss = 1.1503\n",
      "Epoch 0: Total Loss = 1.1608\n",
      "Epoch 0: Total Loss = 1.1657\n",
      "Epoch 0: Total Loss = 1.1435\n",
      "Epoch 0: Total Loss = 1.1740\n",
      "Epoch 0: Total Loss = 1.1675\n",
      "Epoch 0: Total Loss = 1.1780\n",
      "Epoch 0: Total Loss = 1.1663\n",
      "Epoch 0: Total Loss = 1.1303\n",
      "Epoch 0: Total Loss = 1.1535\n",
      "Epoch 0: Total Loss = 1.1749\n",
      "Epoch 0: Total Loss = 1.1128\n",
      "Epoch 0: Total Loss = 1.1563\n",
      "Epoch 0: Total Loss = 1.1826\n",
      "Epoch 0: Total Loss = 1.1383\n",
      "Epoch 0: Total Loss = 1.1355\n",
      "Epoch 0: Total Loss = 1.1430\n",
      "Epoch 0: Total Loss = 1.1593\n",
      "Epoch 0: Total Loss = 1.1503\n",
      "Epoch 0: Total Loss = 1.1560\n",
      "Epoch 0: Total Loss = 1.1571\n",
      "Epoch 0: Total Loss = 1.1457\n",
      "Epoch 0: Total Loss = 1.1352\n",
      "Epoch 0: Total Loss = 1.1465\n",
      "Epoch 0: Total Loss = 1.1249\n",
      "Epoch 0: Total Loss = 1.1629\n",
      "Epoch 0: Total Loss = 1.1520\n",
      "Epoch 0: Total Loss = 1.1551\n",
      "Epoch 0: Total Loss = 1.1815\n",
      "Epoch 0: Total Loss = 1.1349\n",
      "Epoch 0: Total Loss = 1.1581\n",
      "Epoch 0: Total Loss = 1.1293\n",
      "Epoch 0: Total Loss = 1.1779\n",
      "Epoch 0: Total Loss = 1.1589\n",
      "Epoch 0: Total Loss = 1.1323\n",
      "Epoch 0: Total Loss = 1.1576\n",
      "Epoch 0: Total Loss = 1.1425\n",
      "Epoch 0: Total Loss = 1.1492\n",
      "Epoch 0: Total Loss = 1.1565\n",
      "Epoch 0: Total Loss = 1.1778\n",
      "Epoch 0: Total Loss = 1.1384\n",
      "Epoch 0: Total Loss = 1.1607\n",
      "Epoch 0: Total Loss = 1.1687\n",
      "Epoch 0: Total Loss = 1.1520\n",
      "Epoch 0: Total Loss = 1.1738\n",
      "Epoch 0: Total Loss = 1.1739\n",
      "Epoch 0: Total Loss = 1.1430\n",
      "Epoch 0: Total Loss = 1.1699\n",
      "Epoch 0: Total Loss = 1.1396\n",
      "Epoch 0: Total Loss = 1.1805\n",
      "Epoch 0: Total Loss = 1.1221\n",
      "Epoch 0: Total Loss = 1.1380\n",
      "Epoch 0: Total Loss = 1.1569\n",
      "Epoch 0: Total Loss = 1.1082\n",
      "Epoch 0: Total Loss = 1.1363\n",
      "Epoch 0: Total Loss = 1.1236\n",
      "Epoch 0: Total Loss = 1.1270\n",
      "Epoch 0: Total Loss = 1.1320\n",
      "Epoch 0: Total Loss = 1.1662\n",
      "Epoch 0: Total Loss = 1.1235\n",
      "Epoch 0: Total Loss = 1.1271\n",
      "Epoch 0: Total Loss = 1.1438\n",
      "Epoch 0: Total Loss = 1.1525\n",
      "Epoch 0: Total Loss = 1.1949\n",
      "Epoch 0: Total Loss = 1.0982\n",
      "Epoch 0: Total Loss = 1.1257\n",
      "Epoch 0: Total Loss = 1.1581\n",
      "Epoch 0: Total Loss = 1.1859\n",
      "Epoch 0: Total Loss = 1.1175\n",
      "Epoch 0: Total Loss = 1.1795\n",
      "Epoch 0: Total Loss = 1.1320\n",
      "Epoch 0: Total Loss = 1.1481\n",
      "Epoch 0: Total Loss = 1.1604\n",
      "Epoch 0: Total Loss = 1.1600\n",
      "Epoch 0: Total Loss = 1.1547\n",
      "Epoch 0: Total Loss = 1.1617\n",
      "Epoch 0: Total Loss = 1.1708\n",
      "Epoch 0: Total Loss = 1.1247\n",
      "Epoch 0: Total Loss = 1.1474\n",
      "Epoch 0: Total Loss = 1.1707\n",
      "Epoch 0: Total Loss = 1.1532\n",
      "Epoch 0: Total Loss = 1.1346\n",
      "Epoch 0: Total Loss = 1.1416\n",
      "Epoch 0: Total Loss = 1.1586\n",
      "Epoch 0: Total Loss = 1.1399\n",
      "Epoch 0: Total Loss = 1.1503\n",
      "Epoch 0: Total Loss = 1.1376\n",
      "Epoch 0: Total Loss = 1.1671\n",
      "Epoch 0: Total Loss = 1.1594\n",
      "Epoch 0: Total Loss = 1.1599\n",
      "Epoch 0: Total Loss = 1.1404\n",
      "Epoch 0: Total Loss = 1.1828\n",
      "Epoch 0: Total Loss = 1.1356\n",
      "Epoch 0: Total Loss = 1.1561\n",
      "Epoch 0: Total Loss = 1.1013\n",
      "Epoch 0: Total Loss = 1.1462\n",
      "Epoch 0: Total Loss = 1.1594\n",
      "Epoch 0: Total Loss = 1.1418\n",
      "Epoch 0: Total Loss = 1.1451\n",
      "Epoch 0: Total Loss = 1.1843\n",
      "Epoch 0: Total Loss = 1.1332\n",
      "Epoch 0: Total Loss = 1.1535\n",
      "Epoch 0: Total Loss = 1.1697\n",
      "Epoch 0: Total Loss = 1.1283\n",
      "Epoch 0: Total Loss = 1.1141\n",
      "Epoch 0: Total Loss = 1.1783\n",
      "Epoch 0: Total Loss = 1.1794\n",
      "Epoch 0: Total Loss = 1.1433\n",
      "Epoch 0: Total Loss = 1.1721\n",
      "Epoch 0: Total Loss = 1.1535\n",
      "Epoch 0: Total Loss = 1.1578\n",
      "Epoch 0: Total Loss = 1.1669\n",
      "Epoch 0: Total Loss = 1.1379\n",
      "Epoch 0: Total Loss = 1.1455\n",
      "Epoch 0: Total Loss = 1.1703\n",
      "Epoch 0: Total Loss = 1.1407\n",
      "Epoch 0: Total Loss = 1.1642\n",
      "Epoch 0: Total Loss = 1.1375\n",
      "Epoch 0: Total Loss = 1.1626\n",
      "Epoch 0: Total Loss = 1.1322\n",
      "Epoch 0: Total Loss = 1.1447\n",
      "Epoch 0: Total Loss = 1.1383\n",
      "Epoch 0: Total Loss = 1.1321\n",
      "Epoch 0: Total Loss = 1.1631\n",
      "Epoch 0: Total Loss = 1.1826\n",
      "Epoch 0: Total Loss = 1.1480\n",
      "Epoch 0: Total Loss = 1.1631\n",
      "Epoch 0: Total Loss = 1.1266\n",
      "Epoch 0: Total Loss = 1.1712\n",
      "Epoch 0: Total Loss = 1.1226\n",
      "Epoch 0: Total Loss = 1.1463\n",
      "Epoch 0: Total Loss = 1.1549\n",
      "Epoch 0: Total Loss = 1.1607\n",
      "Epoch 0: Total Loss = 1.1561\n",
      "Epoch 0: Total Loss = 1.1207\n",
      "Epoch 0: Total Loss = 1.1358\n",
      "Epoch 0: Total Loss = 1.1670\n",
      "Epoch 0: Total Loss = 1.1633\n",
      "Epoch 0: Total Loss = 1.1584\n",
      "Epoch 0: Total Loss = 1.1009\n",
      "Epoch 0: Total Loss = 1.1805\n",
      "Epoch 0: Total Loss = 1.1606\n",
      "Epoch 0: Total Loss = 1.1407\n",
      "Epoch 0: Total Loss = 1.1478\n",
      "Epoch 0: Total Loss = 1.1860\n",
      "Epoch 0: Total Loss = 1.1599\n",
      "Epoch 0: Total Loss = 1.1233\n",
      "Epoch 0: Total Loss = 1.1356\n",
      "Epoch 0: Total Loss = 1.1700\n",
      "Epoch 0: Total Loss = 1.1425\n",
      "Epoch 0: Total Loss = 1.1513\n",
      "Epoch 0: Total Loss = 1.1422\n",
      "Epoch 0: Total Loss = 1.1289\n",
      "Epoch 0: Total Loss = 1.1456\n",
      "Epoch 0: Total Loss = 1.1209\n",
      "Epoch 0: Total Loss = 1.1578\n",
      "Epoch 0: Total Loss = 1.1615\n",
      "Epoch 0: Total Loss = 1.1587\n",
      "Epoch 0: Total Loss = 1.1800\n",
      "Epoch 0: Total Loss = 1.1524\n",
      "Epoch 0: Total Loss = 1.1243\n",
      "Epoch 0: Total Loss = 1.1882\n",
      "Epoch 0: Total Loss = 1.1387\n",
      "Epoch 0: Total Loss = 1.1356\n",
      "Epoch 0: Total Loss = 1.1659\n",
      "Epoch 0: Total Loss = 1.1523\n",
      "Epoch 0: Total Loss = 1.1672\n",
      "Epoch 0: Total Loss = 1.1429\n",
      "Epoch 0: Total Loss = 1.1309\n",
      "Epoch 0: Total Loss = 1.1999\n",
      "Epoch 0: Total Loss = 1.1640\n",
      "Epoch 0: Total Loss = 1.1420\n",
      "Epoch 0: Total Loss = 1.1521\n",
      "Epoch 0: Total Loss = 1.1802\n",
      "Epoch 0: Total Loss = 1.1646\n",
      "Epoch 0: Total Loss = 1.1326\n",
      "Epoch 0: Total Loss = 1.1723\n",
      "Epoch 0: Total Loss = 1.1516\n",
      "Epoch 0: Total Loss = 1.1680\n",
      "Epoch 0: Total Loss = 1.1570\n",
      "Epoch 0: Total Loss = 1.1794\n",
      "Epoch 0: Total Loss = 1.1571\n",
      "Epoch 0: Total Loss = 1.1705\n",
      "Epoch 0: Total Loss = 1.1745\n",
      "Epoch 0: Total Loss = 1.1605\n",
      "Epoch 0: Total Loss = 1.1592\n",
      "Epoch 0: Total Loss = 1.1298\n",
      "Epoch 0: Total Loss = 1.1513\n",
      "Epoch 0: Total Loss = 1.1570\n",
      "Epoch 0: Total Loss = 1.1331\n",
      "Epoch 0: Total Loss = 1.1161\n",
      "Epoch 0: Total Loss = 1.1146\n",
      "Epoch 0: Total Loss = 1.1450\n",
      "Epoch 0: Total Loss = 1.1494\n",
      "Epoch 0: Total Loss = 1.1690\n",
      "Epoch 0: Total Loss = 1.1605\n",
      "Epoch 0: Total Loss = 1.1600\n",
      "Epoch 0: Total Loss = 1.1750\n",
      "Epoch 0: Total Loss = 1.1625\n",
      "Epoch 0: Total Loss = 1.1136\n",
      "Epoch 0: Total Loss = 1.1694\n",
      "Epoch 0: Total Loss = 1.1422\n",
      "Epoch 0: Total Loss = 1.1627\n",
      "Epoch 0: Total Loss = 1.1655\n",
      "Epoch 0: Total Loss = 1.1472\n",
      "Epoch 0: Total Loss = 1.1450\n",
      "Epoch 0: Total Loss = 1.1731\n",
      "Epoch 0: Total Loss = 1.1754\n",
      "Epoch 0: Total Loss = 1.1661\n",
      "Epoch 0: Total Loss = 1.1888\n",
      "Epoch 0: Total Loss = 1.1681\n",
      "Epoch 0: Total Loss = 1.1690\n",
      "Epoch 0: Total Loss = 1.1879\n",
      "Epoch 0: Total Loss = 1.1397\n",
      "Epoch 0: Total Loss = 1.1684\n",
      "Epoch 0: Total Loss = 1.1691\n",
      "Epoch 0: Total Loss = 1.1632\n",
      "Epoch 0: Total Loss = 1.1202\n",
      "Epoch 0: Total Loss = 1.1324\n",
      "Epoch 0: Total Loss = 1.1531\n",
      "Epoch 0: Total Loss = 1.1301\n",
      "Epoch 0: Total Loss = 1.1398\n",
      "Epoch 0: Total Loss = 1.1292\n",
      "Epoch 0: Total Loss = 1.1451\n",
      "Epoch 0: Total Loss = 1.1714\n",
      "Epoch 0: Total Loss = 1.1224\n",
      "Epoch 0: Total Loss = 1.1372\n",
      "Epoch 0: Total Loss = 1.1816\n",
      "Epoch 0: Total Loss = 1.1679\n",
      "Epoch 0: Total Loss = 1.1425\n",
      "Epoch 0: Total Loss = 1.1533\n",
      "Epoch 0: Total Loss = 1.1488\n",
      "Epoch 0: Total Loss = 1.1658\n",
      "Epoch 0: Total Loss = 1.1776\n",
      "Epoch 0: Total Loss = 1.1652\n",
      "Epoch 0: Total Loss = 1.1483\n",
      "Epoch 0: Total Loss = 1.1294\n",
      "Epoch 0: Total Loss = 1.1779\n",
      "Epoch 0: Total Loss = 1.1820\n",
      "Epoch 0: Total Loss = 1.1288\n",
      "Epoch 0: Total Loss = 1.1735\n",
      "Epoch 0: Total Loss = 1.1522\n",
      "Epoch 0: Total Loss = 1.1445\n",
      "Epoch 0: Total Loss = 1.1734\n",
      "Epoch 0: Total Loss = 1.1587\n",
      "Epoch 0: Total Loss = 1.1166\n",
      "Epoch 0: Total Loss = 1.1136\n",
      "Epoch 0: Total Loss = 1.1870\n",
      "Epoch 0: Total Loss = 1.1741\n",
      "Epoch 0: Total Loss = 1.1554\n",
      "Epoch 0: Total Loss = 1.1264\n",
      "Epoch 0: Total Loss = 1.1645\n",
      "Epoch 0: Total Loss = 1.1590\n",
      "Epoch 0: Total Loss = 1.1195\n",
      "Epoch 0: Total Loss = 1.1209\n",
      "Epoch 0: Total Loss = 1.1611\n",
      "Epoch 0: Total Loss = 1.1560\n",
      "Epoch 0: Total Loss = 1.1388\n",
      "Epoch 0: Total Loss = 1.1517\n",
      "Epoch 0: Total Loss = 1.1540\n",
      "Epoch 0: Total Loss = 1.1523\n",
      "Epoch 0: Total Loss = 1.1370\n",
      "Epoch 0: Total Loss = 1.1690\n",
      "Epoch 0: Total Loss = 1.1450\n",
      "Epoch 0: Total Loss = 1.1606\n",
      "Epoch 0: Total Loss = 1.1288\n",
      "Epoch 0: Total Loss = 1.1502\n",
      "Epoch 0: Total Loss = 1.1412\n",
      "Epoch 0: Total Loss = 1.1376\n",
      "Epoch 0: Total Loss = 1.1376\n",
      "Epoch 0: Total Loss = 1.1163\n",
      "Epoch 0: Total Loss = 1.1197\n",
      "Epoch 0: Total Loss = 1.1571\n",
      "Epoch 0: Total Loss = 1.1494\n",
      "Epoch 0: Total Loss = 1.1656\n",
      "Epoch 0: Total Loss = 1.1471\n",
      "Epoch 0: Total Loss = 1.1801\n",
      "Epoch 0: Total Loss = 1.1560\n",
      "Epoch 0: Total Loss = 1.1783\n",
      "Epoch 0: Total Loss = 1.1363\n",
      "Epoch 0: Total Loss = 1.1558\n",
      "Epoch 0: Total Loss = 1.1705\n",
      "Epoch 0: Total Loss = 1.1540\n",
      "Epoch 0: Total Loss = 1.1644\n",
      "Epoch 0: Total Loss = 1.1618\n",
      "Epoch 0: Total Loss = 1.1098\n",
      "Epoch 0: Total Loss = 1.1444\n",
      "Epoch 0: Total Loss = 1.1507\n",
      "Epoch 0: Total Loss = 1.1159\n",
      "Epoch 0: Total Loss = 1.1572\n",
      "Epoch 0: Total Loss = 1.1696\n",
      "Epoch 0: Total Loss = 1.1716\n",
      "Epoch 0: Total Loss = 1.1730\n",
      "Epoch 0: Total Loss = 1.1677\n",
      "Epoch 0: Total Loss = 1.1397\n",
      "Epoch 0: Total Loss = 1.1227\n",
      "Epoch 0: Total Loss = 1.1567\n",
      "Epoch 0: Total Loss = 1.1769\n",
      "Epoch 0: Total Loss = 1.1452\n",
      "Epoch 0: Total Loss = 1.1895\n",
      "Epoch 0: Total Loss = 1.1885\n",
      "Epoch 0: Total Loss = 1.1629\n",
      "Epoch 0: Total Loss = 1.1557\n",
      "Epoch 0: Total Loss = 1.1308\n",
      "Epoch 0: Total Loss = 1.1691\n",
      "Epoch 0: Total Loss = 1.1474\n",
      "Epoch 0: Total Loss = 1.1369\n",
      "Epoch 0: Total Loss = 1.1712\n",
      "Epoch 0: Total Loss = 1.1610\n",
      "Epoch 0: Total Loss = 1.1811\n",
      "Epoch 0: Total Loss = 1.1649\n",
      "Epoch 0: Total Loss = 1.1443\n",
      "Epoch 0: Total Loss = 1.1746\n",
      "Epoch 0: Total Loss = 1.1780\n",
      "Epoch 0: Total Loss = 1.1725\n",
      "Epoch 0: Total Loss = 1.1498\n",
      "Epoch 0: Total Loss = 1.1465\n",
      "Epoch 0: Total Loss = 1.1729\n",
      "Epoch 0: Total Loss = 1.1887\n",
      "Epoch 0: Total Loss = 1.1655\n",
      "Epoch 0: Total Loss = 1.1302\n",
      "Epoch 0: Total Loss = 1.1769\n",
      "Epoch 0: Total Loss = 1.1432\n",
      "Epoch 0: Total Loss = 1.1290\n",
      "Epoch 0: Total Loss = 1.1427\n",
      "Epoch 0: Total Loss = 1.1634\n",
      "Epoch 0: Total Loss = 1.1315\n",
      "Epoch 0: Total Loss = 1.1716\n",
      "Epoch 0: Total Loss = 1.1798\n",
      "Epoch 0: Total Loss = 1.1562\n",
      "Epoch 0: Total Loss = 1.1591\n",
      "Epoch 0: Total Loss = 1.1460\n",
      "Epoch 0: Total Loss = 1.1475\n",
      "Epoch 0: Total Loss = 1.1540\n",
      "Epoch 0: Total Loss = 1.1774\n",
      "Epoch 0: Total Loss = 1.1397\n",
      "Epoch 0: Total Loss = 1.1444\n",
      "Epoch 0: Total Loss = 1.1435\n",
      "Epoch 0: Total Loss = 1.1816\n",
      "Epoch 0: Total Loss = 1.1445\n",
      "Epoch 0: Total Loss = 1.1543\n",
      "Epoch 0: Total Loss = 1.1896\n",
      "Epoch 0: Total Loss = 1.1457\n",
      "Epoch 0: Total Loss = 1.1275\n",
      "Epoch 0: Total Loss = 1.1635\n",
      "Epoch 0: Total Loss = 1.1416\n",
      "Epoch 0: Total Loss = 1.1564\n",
      "Epoch 0: Total Loss = 1.1197\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "loss_total_step=0.0\n",
    "intermediate_losses = defaultdict(float)\n",
    "num_epochs=1\n",
    "device = torch.device(\"cpu\")  # Specify the GPU device\n",
    "epoch_start=0\n",
    "tokenizer.to(device)\n",
    "all_metrics = []  # List to store metrics for all epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #save_epoch = epoch in [9,19,29,39,49,59,69,79]\n",
    "    \n",
    "    loss_total_epoch=0.0\n",
    "    optimizer_tokenizer.zero_grad()\n",
    "    losses = {}\n",
    "    print(\"epoch {}\".format(epoch))\n",
    "    for i, images in enumerate(loaders['train']):\n",
    "        loss_total_step=0.0\n",
    "        image = images[0]\n",
    "        image = image.unsqueeze(1)\n",
    "        image = image[3:4, :, :, :]\n",
    "        input_image= Variable(image).to(device)  # batch x\n",
    "\n",
    "        optimizer_checkpoint = torch.load('/users/zboucher/iris/src/models/tokenizer/checkpoints/vqvae_epoch30', map_location=device)\n",
    "        optimizer_tokenizer.load_state_dict(optimizer_checkpoint)\n",
    "         #print(input_image.size())\n",
    "        encoder_output = tokenizer.encode(input_image)\n",
    "        losses=tokenizer.compute_loss(input_image)\n",
    "        loss_total_step += losses.loss_total \n",
    "        # Access the individual loss values\n",
    "        if (i+1)%64 == 0: \n",
    "            for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                    intermediate_losses[f\"{str(tokenizer)}/train/{loss_name}\"] = loss_value/64\n",
    "            metrics = {f'{str(Tokenizer)}/train/total_loss': loss_total_step, **intermediate_losses}\n",
    "            print(\"Epoch {}: Total Loss = {:.4f}\".format(epoch, metrics[f'{str(Tokenizer)}/train/total_loss']))\n",
    "            losses = {}\n",
    "            loss_total_step=0\n",
    "        \n",
    "            all_metrics.append(metrics)  # Save metrics for the current epoch to the list\n",
    "\n",
    "\n",
    "# Convert the metrics list to a NumPy array\n",
    "metrics_array = np.array(all_metrics)\n",
    "np.save('/users/zboucher/iris/src/models/tokenizer/metrics_array.npy',metrics_array )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the NPY file\n",
    "data = np.load('/users/zboucher/iris/src/models/tokenizer/metrics_array.npy',allow_pickle=True)\n",
    "\n",
    "# Access and use the data as a NumPy array\n",
    "print(data.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
